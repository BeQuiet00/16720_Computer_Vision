{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2291016bfa10702ead788906211add42",
     "grade": false,
     "grade_id": "cell-f840bb3247da6695",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img align=\"center\" src=\"figures/course.png\" width=\"800\">\n",
    "\n",
    "#                                    16720 (B) Bag of Visual Words - Assignment 2\n",
    "\n",
    "     Instructor: Kris Kitani                       TAs:Paritosh (Lead), Rawal, Yan, Zen, Wen-Hsuan, Qichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "112e3e9d9d76070a09a42ebf28c7c0e7",
     "grade": false,
     "grade_id": "cell-ab714820928ab812",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from skimage import io\n",
    "import skimage.transform\n",
    "import os,time\n",
    "import util\n",
    "import multiprocessing\n",
    "import threading\n",
    "import queue\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3245cc9caf66c97d2949d64b21bbaa4d",
     "grade": false,
     "grade_id": "cell-0943543d897db3c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## For Autograding P4, ensure uploading `trained_conf_matrix.npy` and `trained_system_deep.npz`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3df0d1eeb0e83e225049013b703cb55e",
     "grade": false,
     "grade_id": "cell-17340b02fdb1c6df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Deep Learning Features - An Alternative to ``Bag of Words``\n",
    "\n",
    "As we have discussed in class, another powerful method for scene classification in computer vision is the employment of convolutional neural networks (CNNs) - sometimes referred to generically as $deep learning$. It is important to understand how previously trained (pretrained) networks can be used as another form of feature extraction, and how they relate to classical Bag of Words (BoW) features. We will be covering details on how one chooses the network architecture and training procedures later in the course. For this question, however, we will be asking you to deal with the VGG-16 pretrained network. VGG-16 is a pretrained Convolutional Neural Network (CNN) that has been trained on approximately 1.2 million images from the ImageNet Dataset (``http://image-net.org/index``) by the Visual Geometry Group (VGG) at University of Oxford. The model can classify images into a 1000 object categories (e.g. keyboard, mouse, coffee mug, pencil).\n",
    "\n",
    "One lesson we want you to take away from this exercise is to understand the effectiveness of $deep$ $features$ for general classification tasks within computer vision - even when those features have been previously trained on a different dataset (i.e. ImageNet) and task (i.e. object recognition). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afb19706afe8d0615f44a208e29ff6bd",
     "grade": false,
     "grade_id": "cell-4d7a2d7da9ad6236",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Extracting Deep Features\n",
    "\n",
    "To complete this question, you need to install the ``torchvision`` library from Pytorch, a popular Python-based deep learning library.\n",
    "If you are using the Anaconda package manager (``https://www.anaconda.com/download/``), this can be done with the following command:\n",
    "```\n",
    "            conda install pytorch torchvision -c pytorch\n",
    "```\n",
    "To check that you have installed it correctly, make sure that you can ``import torch`` in a Python interpreter without errors.\n",
    "Please refer to ``https://pytorch.org/`` for more detailed installation instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea96722fcf18543140b0ec3d16bd9e2d",
     "grade": false,
     "grade_id": "cell-1086d0481a485a9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q4.1.1 (25 Points)\n",
    "\n",
    "We want to extract out deep features corresponding to the convolutional layers of theVGG-16 network.  In this problem, we will use the trained weights from the VGG network directly, but implement our own operations. To load the network, use the line\n",
    "```\n",
    "        vgg16 = torchvision.models.vgg16(pretrained=True).double()\n",
    "```\n",
    "followed by ``vgg16.eval()``\n",
    "The latter line ensures that the VGG-16 network is in evaluation mode, not training mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92298155f306c3a6810a855a830ab225",
     "grade": false,
     "grade_id": "cell-0852d8a76345d947",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "path_img = \"./data/aquarium/sun_aztvjgubyrgvirup.jpg\"\n",
    "\n",
    "image = io.imread(path_img)\n",
    "\n",
    "image = image.astype('float')/255\n",
    "\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True).double()\n",
    "\n",
    "vgg16.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28e71afeaf91366424b2b9faf92e5a55",
     "grade": false,
     "grade_id": "cell-8b240fc2fc30262a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We want you to complete a function that is able to output $VGG-16$ network outputs at the **fc7** layer in\n",
    "```\n",
    "    def extract_deep_feature(x,vgg16_weights):\n",
    "```\n",
    "where ``x`` refers to the input image and ``vgg16_weights`` is a structure containing the CNN's network parameters. In this function you will need to write sub-functions ``multichannel_conv2d``, ``relu``, ``max_pool2d``, and ``linear`` corresponding to the fundamental elements of the CNN: multi-channel convolution, rectified linear units (ReLU), max pooling, and fully-connected weighting.\n",
    "\n",
    "We have provided a helper function ``util.get_VGG16_weights()`` that extracts the weight parameters of VGG-16 and its meta information. The returned variable is a numpy array of shape $L\\times 3$, where $L$ is the number of layers in VGG-16. The first column of each row is a string indicating the layer type. The second/third columns may contain the learned weights and biases, or other meta-information (\\eg kernel size of max-pooling). Please refer to the function docstring for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "126bd1545c46831c642e8428f6660a7c",
     "grade": false,
     "grade_id": "cell-5cbd9317d3fa3557",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In order to build the ``extract_deep_feature`` function, you should run a for-loop through each layer index until layer **fc7**, which corresponds to **the second linear layer** (Refer to VGG structure to see where **fc7** is). **Remember**: the output of the preceding layer should be passed through as an input to the next.\n",
    "\n",
    "Details on the sub-functions needed for the ``extract_deep_feature`` function can be found below.\n",
    "\n",
    "Please use ``scipy.ndimage.convolve`` and ``numpy`` functions to implement these functions instead of using pytorch. Please keep speed in mind when implementing your function, for example, using double for loop over pixels is not a good idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "265efb8ebc1b1970c7cdb98ffd8c36b3",
     "grade": false,
     "grade_id": "cell-536cfed312995b11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "``multichannel_conv2d(x,weight,bias)``:\n",
    "\n",
    "a function that will perform multi-channel 2D convolution which can be defined as follows, \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y}^{(j)} = \\sum_{k=1}^{K} \\begin{bmatrix} \\mathbf{x}^{(k)} * \\mathbf{h}^{(j,k)} \\end{bmatrix} + \\mathbf{b}[j] \n",
    "\\end{equation}\n",
    "\n",
    "where $*$ denotes $2D$ convolution, $\\mathbf{x} = \\{ \\mathbf{x}^{(k)} \\}_{k=1}^{K}$ is our vectorized $K$-channel input signal, $\\mathbf{h} = \\{ \\mathbf{h}^{(j,k)} \\}_{k=1,j=1}^{K,J}$ is our $J \\times K$ set of vectorized convolutional filters and $\\mathbf{r} = \\{ \\mathbf{y}^{(j)} \\}_{j=1}^{J}$ is our $J$ channel vectorized output response. Further, unlike traditional single-channel convolution CNNs often append a bias vector $\\mathbf{b}$ whose $J$ elements are added to the $J$ channels of the output response. \n",
    "\n",
    "To implement ``multichannel_conv2d``, you can use the Scipy convolution function directly with for loops to cycle through the filters and channels (``scipy.ndimage.convolve()``). All the necessary details concerning the number of filters ($J$), number of channels ($K$), filter weights ($\\mathbf{h}$) and biases ($\\mathbf{b}$) can be inferred from the shapes/dimensions of the weights and biases. Notice that pytorch's convolution function actually does correlation, so to get similar answer as pytroch with scipy, you need to flip the kernel on both axes using ``np.flip()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfafc16377dad8df84358ed123409a7d",
     "grade": false,
     "grade_id": "cell-39e262deb285001e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multichannel_conv2d(x,weight,bias):\n",
    "    '''\n",
    "    Performs multi-channel 2D convolution.\n",
    "\n",
    "    [input]\n",
    "    * x: numpy.ndarray of shape (H,W,input_dim)\n",
    "    * weight: numpy.ndarray of shape (output_dim,input_dim,kernel_size,kernel_size)\n",
    "    * bias: numpy.ndarray of shape (output_dim)\n",
    "\n",
    "    [output]\n",
    "    * feat: numpy.ndarray of shape (H,W,output_dim)\n",
    "    '''\n",
    "    h, w, input_dims = x.shape\n",
    "    output_dims = weight.shape[0]\n",
    "    final_res = np.zeros((h, w, output_dims))\n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> for 2D convolution we need to use np.fliplr and np.flipud\n",
    "    2.> can use scipy.ndimage.convolve with the flipped kernel\n",
    "    3.> don't forget to add the bias\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    for i in range(output_dims):\n",
    "        kernel = weight[i, :, :, :]\n",
    "        out_feat = np.zeros((h, w))\n",
    "        for j in range(input_dims):\n",
    "            out_feat += scipy.ndimage.convolve(x[:, :, j], kernel[j, ::-1, ::-1], mode='constant', cval=0)\n",
    "        final_res[:, :, i] = out_feat + bias[i]\n",
    "    \n",
    "    # raise NotImplementedError()\n",
    "    return final_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ae1c551b9359d94adf085873f98e4ba",
     "grade": false,
     "grade_id": "cell-244ad83440a13a11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "```relu(x)```:\n",
    "\n",
    "a function that shall perform the Rectified Linear Unit (ReLU) which can be defined mathematically as,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mbox{ReLU}(x) = \\max_{x}(x, 0)\n",
    "\\end{equation}\n",
    "\n",
    "and is applied independently to each element of the matrix/vector $x$ passed to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d0e9f9b71377bf8a52d6526abf7834f",
     "grade": false,
     "grade_id": "cell-9129e8637bed35d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    '''\n",
    "    Rectified linear unit.\n",
    "\n",
    "    [input]\n",
    "    * x: numpy.ndarray\n",
    "\n",
    "    [output]\n",
    "    * y: numpy.ndarray\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    y = np.maximum(x, 0)\n",
    "    \n",
    "    # raise NotImplementedError()\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "850072d50afe0d0486db01d3c3dc028e",
     "grade": false,
     "grade_id": "cell-bbe9e052ed717774",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "``max_pool2d(x,size):``\n",
    "\n",
    "a function that shall perform max pooling over $x$ using a receptive field of size $size$ $\\times$ $size$ (we assume a square receptive field here for simplicity).\n",
    "\n",
    "  If the function receives a multi-channel input, then it should apply the max pooling operation across each input channel independently.\n",
    "  \n",
    "(Hint: making use of smart array indexing can drastically speed up the code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc3d9525fa5233cbcb1a2288e5408d1d",
     "grade": false,
     "grade_id": "cell-c1438c61d632ab1e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def max_pool2d(x,size):\n",
    "    '''\n",
    "    2D max pooling operation.\n",
    "\n",
    "    [input]\n",
    "    * x: numpy.ndarray of shape (H,W,input_dim)\n",
    "    * size: pooling receptive field\n",
    "\n",
    "    [output]\n",
    "    * y: numpy.ndarray of shape (H/size,W/size,input_dim)\n",
    "    '''\n",
    "    h, w, dims = x.shape\n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> estimate the shape you need to apply the pooling operation.\n",
    "    2.> We can smart fill the padding with np.nan and then use np.nanmax to select the max (avoiding nan)\n",
    "    3.> We can input the grid (start_x:end_x, start_y:end_y, dim) as smart array indexing to np.nanmax\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    h, w, input_dim = x.shape\n",
    "    pooled_arr = np.zeros((h//size, w//size, input_dim))\n",
    "    for i in range(h//size):\n",
    "        for j in range(w//size):\n",
    "            sub_x = x[i*size:(i+1)*size, j*size:(j+1)*size, :]\n",
    "            pooled_arr[i, j, :] = np.max(sub_x, axis=(0, 1))\n",
    "    \n",
    "    # raise NotImplementedError()\n",
    "    return pooled_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a30a5e18db579607d17478cb28066cc",
     "grade": false,
     "grade_id": "cell-a0d2f5e224adba0f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "``linear(x,W,b):``\n",
    "\n",
    "a function that will compute a node vector where each element is a linear combination of the input nodes, written as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y}[j] = \\sum_{k=1}^{K}\\mathbf{W}[j,k] \\mathbf{x}[k] + \\mathbf{b}[j] \n",
    "\\end{equation}\n",
    "\n",
    "or more succinctly in vector form as $\\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}$ - where $\\mathbf{x}$ is the $(K \\times 1)$ input node vector, $\\mathbf{W}$ is the $(J \\times K)$ weight matrix, $\\mathbf{b}$ is the $(J \\times 1)$ bias vector and $\\mathbf{y}$ is the~$(J \\times 1)$ output node vector.\n",
    "\n",
    "You should not need for-loops to implement this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83ed8dacb1fb983d4d1f336de7601abc",
     "grade": false,
     "grade_id": "cell-aee1e87151903086",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear(x,W,b):\n",
    "    '''\n",
    "    Fully-connected layer.\n",
    "\n",
    "    [input]\n",
    "    * x: numpy.ndarray of shape (input_dim)\n",
    "    * weight: numpy.ndarray of shape (output_dim,input_dim)\n",
    "    * bias: numpy.ndarray of shape (output_dim)\n",
    "\n",
    "    [output]\n",
    "    * y: numpy.ndarray of shape (output_dim)\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    y = np.dot(W, x) + b\n",
    "    \n",
    "    # raise NotImplementedError()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "146fa6565ac543a5086ff5c24eeb6585",
     "grade": false,
     "grade_id": "cell-54047ba1b7802e8e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You should ignore all ``DropoutLayer`` you encounter; they're functional only during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a6f29fbda60148eaaac3fb8e7f4b89d",
     "grade": false,
     "grade_id": "cell-5491f084ad832f2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "VGG-16 assumes that all input imagery to the network is resized to $224 \\times 224$ with the three color channels preserved (use ``skimage.transform.resize()`` to do this before passing any imagery to the network). And be sure to normalize the image using suggested mean and std before extracting the feature:\n",
    "```\n",
    "                                        mean=[0.485,0.456,0.406]}\n",
    "                                        std=[0.229,0.224,0.225]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9656d91025bba1426dd161ada89c41ec",
     "grade": false,
     "grade_id": "cell-7d0ed28aa0f1bffb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    '''\n",
    "    Preprocesses the image to load into the prebuilt network.\n",
    "\n",
    "    [input]\n",
    "    * image: numpy.ndarray of shape (H,W,3)\n",
    "\n",
    "    [output]\n",
    "    * image_processed: torch.array of shape (3,H,W)\n",
    "    '''\n",
    "\n",
    "    # ----- TODO -----\n",
    "    \n",
    "    if(len(image.shape) == 2):\n",
    "        image = np.stack((image, image, image), axis=-1)\n",
    "\n",
    "    if(image.shape == 3 and image.shape[2] == 1):\n",
    "        image = np.concatenate((image, image, image), axis=-1)\n",
    "\n",
    "    if(image.shape[2] == 4):\n",
    "        image = image[:, :, 0:3]\n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> Resize the image (look into skimage.transform.resize)\n",
    "    2.> normalize the image\n",
    "    3.> convert the image from numpy to torch\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # normalize the image and convert it to tensor\n",
    "    image = skimage.transform.resize(image, (224, 224))\n",
    "    normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    preprocess = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), normalize])\n",
    "    image_processed = preprocess(image)\n",
    "\n",
    "    # raise NotImplementedError()\n",
    "    return image_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8725f2cb95e93f82c875bce123ee6c4c",
     "grade": false,
     "grade_id": "cell-1cede50aa78672ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For efficiency you should check that each sub-function is working properly before putting them all together - otherwise it will be hard to track any errors. To compare your implementation with pytroch, you should compare the extracted features between your ``extract_deep_feature``  and the pre-trained VGG-16 network.  ``evaluate_deep_extractor`` should come in handy in comparing the result of the two extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c391c1be80d17baab19771e8a946935",
     "grade": false,
     "grade_id": "cell-27d0fcec8607c9f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_deep_feature(x, vgg16_weights):\n",
    "    '''\n",
    "    Extracts deep features from the given VGG-16 weights.\n",
    "\n",
    "    [input]\n",
    "    * x: numpy.ndarray of shape (H,W,3)\n",
    "    * vgg16_weights: list of shape (L,3)\n",
    "\n",
    "    [output]\n",
    "    * feat: numpy.ndarray of shape (K)\n",
    "    '''\n",
    "    \n",
    "    feat = np.copy(x)\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # feat = skimage.transform.resize(x, (224, 224))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    feat = (feat - mean) / std\n",
    "    linear_count = 0\n",
    "    for layer in vgg16_weights:\n",
    "        if layer[0] == 'conv2d':\n",
    "            feat = multichannel_conv2d(feat, layer[1], layer[2])\n",
    "        elif layer[0] == 'relu':\n",
    "            feat = relu(feat)\n",
    "            if linear_count == 2:\n",
    "                break\n",
    "        elif layer[0] == 'maxpool2d':\n",
    "            feat = max_pool2d(feat, layer[1])\n",
    "        elif layer[0] == 'linear':\n",
    "            if len(feat.shape) != 1:\n",
    "                feat = np.swapaxes(feat, 0, 2)\n",
    "                feat = np.swapaxes(feat, 1, 2)\n",
    "            feat = feat.flatten()\n",
    "            feat = linear(feat, layer[1], layer[2])\n",
    "            linear_count += 1\n",
    "    \n",
    "    # raise NotImplementedError()\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1957f46435c9183a7cb810f8b189c160",
     "grade": false,
     "grade_id": "cell-83df22bcc6adc770",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_deep_extractor(img, vgg16):\n",
    "    '''\n",
    "    Evaluates the deep feature extractor for a single image.\n",
    "\n",
    "    [input]\n",
    "    * image: numpy.ndarray of shape (H,W,3)\n",
    "    * vgg16: prebuilt VGG-16 network.\n",
    "\n",
    "    [output]\n",
    "    * diff: difference between the two feature extractor's result\n",
    "    '''\n",
    "    \n",
    "    vgg16_weights = util.get_VGG16_weights()\n",
    "    img_torch = preprocess_image(img)\n",
    "    \n",
    "    feat = extract_deep_feature(np.transpose(img_torch.numpy(), (1,2,0)), vgg16_weights)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vgg_classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-3])\n",
    "        vgg_feat_feat = vgg16.features(img_torch[None, ])\n",
    "        vgg_feat_feat = vgg_classifier(vgg_feat_feat.flatten())\n",
    "    \n",
    "    return np.sum(np.abs(vgg_feat_feat.numpy() - feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.07748921 -4.14021829 -4.59666275 ... -0.56953101 -1.7098249\n",
      "  0.87203393]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10713.309337739245"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vgg16 = torchvision.models.vgg16(pretrained=True).double()\n",
    "# vgg16.eval()\n",
    "\n",
    "# path_img = \"./data/kitchen/sun_abtdlltafvpypcff.jpg\"\n",
    "# img = io.imread(path_img)\n",
    "# img = img.astype('float')/255\n",
    "# evaluate_deep_extractor(img, vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f71d40e5082c99099739ccd3b77548d",
     "grade": false,
     "grade_id": "cell-b841ba750b9c0b46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Building a Visual Recognition System: Revisited\n",
    "\n",
    "We want to compare how useful deep features are in a visual recognition system. Since the speed of the function ``` scipy.ndimage.convolve``` is not ideal, you can use the pytroch VGG-16 network directly (refer to the helper function ```evaluate_deep_extractor``` on how to use the pre-trained network as feature extractor).\n",
    "\n",
    "#### Q4.2.1 (5 Points Autograder + WriteUp):\n",
    "Implement the functions\n",
    "```\n",
    "                    def build_recognition_system(vgg16):\n",
    "```\n",
    "and\n",
    "```\n",
    "                    def eval_recognition_system(vgg16)}\n",
    "```\n",
    "both of which takes the pretrained VGG-16 network as the input arguments. \n",
    "\n",
    "The former function should produce ``trained_system_deep.npz`` as the output. \n",
    "\n",
    "Included will be:\n",
    "* $features$ : a $N \\times  K$ matrix containing all the deep features of the $N$ training images in the data set.\n",
    "* $labels$ : an $N$ vector containing the labels of each of the images. ($features[i]$ will correspond to label $labels[i]$).\n",
    "\n",
    "The latter function should produce the confusion matrix, as with the previous question.\n",
    "\n",
    "Instead of using the histogram intersection similarity, write a function to just use the negative Euclidean distance (as larger values are more similar).\n",
    "\n",
    "**Report the confusion matrix and accuracy for your results in your write-up. Can you comment in your writeup on whether the results are better or worse than classical BoW - why do you think that is?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "addc54e85c5948aba2bcf80cc2072e86",
     "grade": false,
     "grade_id": "cell-998a996bb091f4c9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_image_feature(args):\n",
    "    '''\n",
    "    Extracts deep features from the prebuilt VGG-16 network.\n",
    "    This is a function run by a subprocess.\n",
    "    [input]\n",
    "    * i: index of training image\n",
    "    * image_path: path of image file\n",
    "    * vgg16: prebuilt VGG-16 network.\n",
    "    \n",
    "    [output]\n",
    "    * feat: evaluated deep feature\n",
    "    '''\n",
    "    i, image_path, vgg16 = args\n",
    "    image = io.imread(image_path)\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> Think along the lines of evaluate_deep_extractor\n",
    "    '''\n",
    "    # ----- TODO -----\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    img_torch = preprocess_image(img)\n",
    "    with torch.no_grad():\n",
    "        vgg_classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-3])\n",
    "        vgg_feat_feat = vgg16.features(img_torch[None, ])\n",
    "        vgg_feat_feat = vgg_classifier(vgg_feat_feat.flatten())\n",
    "    \n",
    "    # raise NotImplementedError()\n",
    "    return [i,vgg_feat_feat.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "690e046304f5925ce2bbfbae43d83828",
     "grade": false,
     "grade_id": "cell-df193a9dbf349af9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 %\n",
      "1 %\n",
      "2 %\n",
      "3 %\n",
      "4 %\n",
      "5 %\n",
      "6 %\n",
      "7 %\n",
      "8 %\n",
      "9 %\n",
      "10 %\n",
      "11 %\n",
      "12 %\n",
      "13 %\n",
      "14 %\n",
      "15 %\n",
      "16 %\n",
      "17 %\n",
      "18 %\n",
      "19 %\n",
      "20 %\n",
      "21 %\n",
      "22 %\n",
      "23 %\n",
      "24 %\n",
      "25 %\n",
      "26 %\n",
      "27 %\n",
      "28 %\n",
      "28 %\n",
      "30 %\n",
      "31 %\n",
      "32 %\n",
      "33 %\n",
      "34 %\n",
      "35 %\n",
      "36 %\n",
      "37 %\n",
      "38 %\n",
      "39 %\n",
      "40 %\n",
      "41 %\n",
      "42 %\n",
      "43 %\n",
      "44 %\n",
      "45 %\n",
      "46 %\n",
      "47 %\n",
      "48 %\n",
      "49 %\n",
      "50 %\n",
      "51 %\n",
      "52 %\n",
      "53 %\n",
      "54 %\n",
      "55 %\n",
      "56 %\n",
      "56 %\n",
      "57 %\n",
      "59 %\n",
      "60 %\n",
      "61 %\n",
      "62 %\n",
      "63 %\n",
      "64 %\n",
      "65 %\n",
      "66 %\n",
      "67 %\n",
      "68 %\n",
      "69 %\n",
      "70 %\n",
      "71 %\n",
      "72 %\n",
      "73 %\n",
      "74 %\n",
      "75 %\n",
      "76 %\n",
      "77 %\n",
      "78 %\n",
      "79 %\n",
      "80 %\n",
      "81 %\n",
      "82 %\n",
      "83 %\n",
      "84 %\n",
      "85 %\n",
      "86 %\n",
      "87 %\n",
      "88 %\n",
      "89 %\n",
      "90 %\n",
      "91 %\n",
      "92 %\n",
      "93 %\n",
      "94 %\n",
      "95 %\n",
      "96 %\n",
      "97 %\n",
      "98 %\n",
      "99 %\n",
      "done (1000, 4096)\n"
     ]
    }
   ],
   "source": [
    "def build_recognition_system(vgg16, num_workers=2):\n",
    "    '''\n",
    "    Creates a trained recognition system by generating training features from all training images.\n",
    "\n",
    "    [input]\n",
    "    * vgg16: prebuilt VGG-16 network.\n",
    "    * num_workers: number of workers to process in parallel\n",
    "\n",
    "    [saved]\n",
    "    * features: numpy.ndarray of shape (N,K)\n",
    "    * labels: numpy.ndarray of shape (N)\n",
    "    '''\n",
    "\n",
    "    train_data = np.load(\"./data/train_data.npz\", allow_pickle=True)\n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> Similar approach as Q1.2.2 and Q3.1.1 (create an argument list and use multiprocessing)\n",
    "    2.> Keep track of the order in which input is given to multiprocessing\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    image_names = train_data['files']\n",
    "    train_labels = train_data['labels']\n",
    "    num_images = image_names.shape[0]\n",
    "    \n",
    "    results = []\n",
    "    for i in range(num_images):\n",
    "        file_path = './data/' + image_names[i]\n",
    "        arg = [i, file_path, vgg16]\n",
    "        results.append(get_image_feature(arg))\n",
    "        if (i%10 == 0): print(int(i/num_images*100), '%')\n",
    "    \n",
    "    # raise NotImplementedError()\n",
    "    # ordered_features = [None] * len(features)\n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> reorder the features to their correct place as input\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    ordered_features = np.array([result[1] for result in results])\n",
    "    labels = np.copy(train_labels)\n",
    "    \n",
    "    # raise NotImplementedError()\n",
    "    print(\"done\", ordered_features.shape)\n",
    "    \n",
    "    np.savez('trained_system_deep.npz', features=ordered_features, labels=labels)\n",
    "    \n",
    "# vgg16 = torchvision.models.vgg16(pretrained=True).double()\n",
    "# vgg16.eval()\n",
    "# build_recognition_system(vgg16, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d99e8eff30887e39239fe479c99a33a6",
     "grade": false,
     "grade_id": "cell-c7bb13cc505da9dc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained features shape:  (1000, 4096)\n",
      "0 %\n",
      "6 %\n",
      "12 %\n",
      "18 %\n",
      "25 %\n",
      "31 %\n",
      "37 %\n",
      "43 %\n",
      "50 %\n",
      "56 %\n",
      "62 %\n",
      "68 %\n",
      "75 %\n",
      "81 %\n",
      "87 %\n",
      "93 %\n",
      "Predicted labels shape:  (160,)\n",
      "accuracy =  0.11875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0., 14.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 18.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 25.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 26.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 13.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 24.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 21.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 19.]]),\n",
       " 0.11875)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def distance_to_set(word_hist, histograms):\n",
    "    '''\n",
    "    Compute similarity between a histogram of visual words with all training image histograms.\n",
    "\n",
    "    [input]\n",
    "    * word_hist: numpy.ndarray of shape (K)\n",
    "    * histograms: numpy.ndarray of shape (N,K)\n",
    "\n",
    "    [output]\n",
    "    * sim: numpy.ndarray of shape (N)\n",
    "    '''\n",
    "    '''\n",
    "    HINTS:\n",
    "    (1) Consider A = [0.1,0.4,0.5] and B = [[0.2,0.3,0.5],[0.8,0.1,0.1]] then \\\n",
    "        similarity between element A and set B could be represented as [[0.1,0.3,0.5],[0.1,0.1,0.1]]   \n",
    "    '''\n",
    "    # ----- TODO -----\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    sim = np.linalg.norm(word_hist - histograms, axis=1)\n",
    "    \n",
    "    # raise NotImplementedError()\n",
    "    return sim\n",
    "\n",
    "def evaluate_recognition_one_image(args):\n",
    "    # helper_function\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    i, file_path, trained_features, trained_labels, vgg16 = args\n",
    "    \n",
    "    image = skimage.io.imread(file_path)\n",
    "    image = image.astype('float') / 255\n",
    "    \n",
    "    # get similarity scores\n",
    "    image_feature = get_image_feature([i, file_path, vgg16])[1]\n",
    "    # print(image_hist)\n",
    "    similarity_scores = distance_to_set(image_feature, trained_features)\n",
    "    # print(similarity_scores)\n",
    "    \n",
    "    # find the index of the highest score\n",
    "    nearest_image_idx = np.where(similarity_scores == np.max(similarity_scores))[0][0]\n",
    "    pred_label = trained_labels[nearest_image_idx]\n",
    "    \n",
    "    # raise NotImplementedError()\n",
    "    return [i, pred_label]\n",
    "\n",
    "def evaluate_recognition_system(vgg16, num_workers=2):\n",
    "    '''\n",
    "    Evaluates the recognition system for all test images and returns the confusion matrix.\n",
    "\n",
    "    [input]\n",
    "    * vgg16: prebuilt VGG-16 network.\n",
    "    * num_workers: number of workers to process in parallel\n",
    "\n",
    "    [output]\n",
    "    * conf: numpy.ndarray of shape (8,8)\n",
    "    * accuracy: accuracy of the evaluated system\n",
    "    '''\n",
    "    '''\n",
    "    HINTS:\n",
    "    (1) Students can write helper functions (in this cell) to use multi-processing\n",
    "    '''\n",
    "    test_data = np.load(\"./data/test_data.npz\", allow_pickle=True)\n",
    "\n",
    "    # ----- TODO -----\n",
    "    xtrained_system = np.load(\"trained_system_deep.npz\", allow_pickle=True)\n",
    "    image_names = test_data['files']\n",
    "    test_labels = test_data['labels']\n",
    "    num_images = image_names.shape[0]\n",
    "\n",
    "    trained_features = xtrained_system['features']\n",
    "    trained_labels = xtrained_system['labels']\n",
    "\n",
    "    print(\"Trained features shape: \", trained_features.shape)\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> [Important] Can write a helper function in this cell of jupyter notebook for multiprocessing\n",
    "    \n",
    "    2.> Helper function will compute the vgg features for test image (get_image_feature) and find closest\n",
    "        matching feature from trained_features.\n",
    "    \n",
    "    3.> Since trained feature is of shape (N,K) -> smartly repeat the test image feature N times (bring it to\n",
    "        same shape as (N,K)). Then we can simply compute distance in a vectorized way.\n",
    "    \n",
    "    4.> Distance here can be sum over (a-b)**2\n",
    "    \n",
    "    5.> np.argmin over distance can give the closest point\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    results = []\n",
    "    for i in range(num_images):\n",
    "        file_path = './data/' + image_names[i]\n",
    "        file_label = test_labels[i]\n",
    "        args = [i, file_path, trained_features, trained_labels, vgg16]\n",
    "        results.append(evaluate_recognition_one_image(args))\n",
    "        if (i%10 == 0): print(int(i/num_images*100), '%')\n",
    "    \n",
    "    # raise NotImplementedError()\n",
    "    \n",
    "    ordered_labels = np.array([result[1] for result in results])\n",
    "    ordered_labels = np.array(ordered_labels, dtype=int)\n",
    "    print(\"Predicted labels shape: \", ordered_labels.shape)\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> Compute the confusion matrix (8x8)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    conf_matrix = np.zeros((8, 8))\n",
    "    for idx in range(len(results)):\n",
    "        i = test_labels[idx]\n",
    "        j = ordered_labels[idx]\n",
    "        conf_matrix[i, j] += 1\n",
    "    accuracy = np.diag(conf_matrix).sum() / conf_matrix.sum()\n",
    "    \n",
    "    print(\"accuracy = \", accuracy)\n",
    "    \n",
    "    # raise NotImplementedError()\n",
    "    \n",
    "    np.save(\"./trained_conf_matrix.npy\",conf_matrix)\n",
    "    return conf_matrix, accuracy\n",
    "    # pass\n",
    "    \n",
    "# vgg16 = torchvision.models.vgg16(pretrained=True).double()\n",
    "# vgg16.eval()\n",
    "# evaluate_recognition_system(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "139b99f8d6775c060c31a5fd340b9143",
     "grade": true,
     "grade_id": "q_4_1_1",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c7cf16f5994d47f23c4c127f7a318a5",
     "grade": true,
     "grade_id": "q_4_1_2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce812af4b046483ab1e43c6792b4448d",
     "grade": true,
     "grade_id": "q_4_1_3",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19d4948aa8f9c3ce2b4181d9919d68b1",
     "grade": true,
     "grade_id": "q_4_1_4",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02a0e96d948d22209c3b3a5626ae40b6",
     "grade": true,
     "grade_id": "q_4_1_5",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7555970fe39f565b777d4c8f37478396",
     "grade": true,
     "grade_id": "q_4_1_6",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40e815d75f7923b2bc81403f5cf53bc8",
     "grade": true,
     "grade_id": "q_4_2_2",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
