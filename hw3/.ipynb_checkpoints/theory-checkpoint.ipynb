{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "025d6187",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc25b0a4ba6f32aecb61b1c243e16de3",
     "grade": false,
     "grade_id": "cell-63d8d04a820a9a31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img align=\"center\" src=\"figures/course.png\" width=\"800\">\n",
    "\n",
    "#                                    16720 (B) Neural Networks for Recognition - Assignment 3\n",
    "\n",
    "     Instructor: Kris Kitani                       TAs: Qichen(Lead), Paritosh, Rawal, Yan, Zen, Wen-Hsuan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e0b61",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0598b3ff2a94d3ff38afa7b1c21e1c07",
     "grade": false,
     "grade_id": "cell-f9a9cc792e95e7c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Submission Instructions:\n",
    "\n",
    "1. Submit the PDF version of `theory.ipynb` to HW3:PDF. The `theory.ipynb` should include **ALL the writeup answers AND ALL the screenshots of code** specifically required in questions **from Q1 to Q7**. This section will be manually Graded.\n",
    "\n",
    "2. Submit `q2.ipynb`, `q3.ipynb`, `q5.ipynb` to HW3:Code. Please do not submit other jupyter notebooks as they will not be autograded. Submitting them may cause running time out. (`q5.ipynb` is optional for extra credits)\n",
    "\n",
    "**The Appendix section at the end of this file would help you on questions P1 and P2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee6744",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "544429557fd95d21890d2229c74a7ff4",
     "grade": false,
     "grade_id": "cell-e4414269f8c1645b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q1 Theory Questions  (45 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab18847",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d8aa00056b3211954c375ea35e715b2",
     "grade": false,
     "grade_id": "cell-cf1227140b9ba295",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.1 (4 Points WriteUp)\n",
    "Prove that softmax is invariant to translation, that is \n",
    "$$softmax(x) = softmax(x + c) \\qquad \\forall c \\in \\mathbb{R}$$\n",
    "Softmax is defined as below, for each index $i$ in a vector $x$.\n",
    "$$softmax(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$\n",
    "Often we use $c = − \\max x_i$. Why is that a good idea? (Tip: consider the range of values that numerator will have with $c = 0$ and $c = − \\max x_i$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22802e66",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3941e91facfc23f3fe531c8367d945e8",
     "grade": true,
     "grade_id": "cell-974a0ae8f06d6586",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "With the input change from x to x+c, both numerator and denominator of the function $softmax(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $ will be multiplied by $e^{c}$.\n",
    "\n",
    "Thus, softmax if invariant to translation.\n",
    "\n",
    "With $c = − \\max x_i$, $e^{x_max-c}=1$, which will avoid zero denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b144ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e0ddcad64e1974a1b283ab4b960f3b8",
     "grade": false,
     "grade_id": "cell-948036aa5862be04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.2\n",
    "\n",
    "Softmax can be written as a three step processes, with $s_i = e^{x_i}$ , $S=\\sum_i s_i$ and $softmax(x_i)= \\frac{1}{S} s_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361cb29",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c9340fc4070513afbb444b1d1563eed",
     "grade": false,
     "grade_id": "cell-52cff4a1493a7fff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.2.1 (1 point WriteUp)\n",
    "As $x \\in \\mathbb{R}^d$, what are the properties of $softmax(x)$, namely what is the range of each element? What is the sum over all elements?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83138ae7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67459c94158ef00d66c43c9896caf245",
     "grade": true,
     "grade_id": "cell-56521164bb7b5976",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The range of each element will be between 0 to 1. And the sum of all element will be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3f3cff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28faddaa495b54e962d322d74f319c7a",
     "grade": false,
     "grade_id": "cell-3a8e1905906ce40a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.2.2 (1 point WriteUp)\n",
    "One could say that ”softmax takes an arbitrary real valued vector $x$ and turns it into a ___”. Please think about a short phrase to fill in ___."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b6fa2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13459335a8f97ab284dfde9c24523418",
     "grade": true,
     "grade_id": "cell-6044a09dfc72a819",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Probablity diatribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec2b12",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ac41c022a0f310b880438d66a81126e",
     "grade": false,
     "grade_id": "cell-501239c17b8d9fc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.2.3 (1 point WriteUp)\n",
    "Can you see the role of each step in the multi-step process now? Explain them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974cc8b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7279aaddce308bb0f897a9dd5e9d6ab",
     "grade": true,
     "grade_id": "cell-93d39e169a1584b1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The first step turns all values into positive values.\n",
    "\n",
    "The second step computes the sum of all values.\n",
    "\n",
    "The third step normalize each values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b381cd7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f0139c980e6271bdcf924a4c4a09cca",
     "grade": false,
     "grade_id": "cell-182ef4991d435a9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.3 (3 points WriteUp)\n",
    "Show that multi-layer neural networks without a non-linear activation function are equivalent to linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a3792",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "152725c4c687af9a3cbc311a9bf689e8",
     "grade": true,
     "grade_id": "cell-31c734ab09fd4344",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "With $y_1=W_1*x_1+b_1$ and $y_2=W_2*y_1+b_2$,\n",
    "\n",
    "$y_2=W_2*W_1*x_1+W_2*b_1+b_2=W_3*x_1+b_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce15f8c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c65af3aee0fcba80878ceaa805e2c911",
     "grade": false,
     "grade_id": "cell-4e4a1870feae2779",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.4 (4 points WriteUp) \n",
    "Given the sigmoid activation function $\\sigma(x) = \\frac{1}{1+e^{-x}}$ , derive the gradient of the sigmoid function and show that it can be written as a function of $\\sigma(x)$ (without having access to $x$ directly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36bc4e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d7be6df91900e029bbd3c1e75e45be6",
     "grade": true,
     "grade_id": "cell-bb3ef4dae24f0472",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$-(1+e^{-x})^{-2}*e^{-x}*(-1)$\n",
    "\n",
    "$=(1+e^{-x})^{-2}*e^{-x}$\n",
    "\n",
    "$=(1+e^{-x})^{-1}*(1+e^{-x})^{-1}*e^{-x}$\n",
    "\n",
    "$=\\sigma(x)*(1-\\sigma(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb96ea82",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3dff66146a32c6b75e3b0d8a1dc75740",
     "grade": false,
     "grade_id": "cell-c77fa30dd0533616",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.5 (12 points WriteUp)\n",
    "\n",
    "Given $y = W x + b$ (or $y_j = \\sum_{i=1}^d  x_{i} W_{ji} + b_j$), and the gradient of some loss $J$ with respect $y$, show how to get $\\frac{\\partial J}{\\partial W}$, $\\frac{\\partial J}{\\partial x}$ and $\\frac{\\partial J}{\\partial b}$. Be sure to do the derivatives with scalars and re-form the matrix form afterwards. Here are some notional suggestions.\n",
    "$$ \\frac{\\partial J}{\\partial y} = \\delta \\in \\mathbb{R}^{k \\times 1} \\quad W \\in \\mathbb{R}^{k \\times d} \\quad x \\in \\mathbb{R}^{d \\times 1} \\quad b \\in \\mathbb{R}^{k \\times 1}$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb05c3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b66294c9dda4ab13c5d027e6bc82e64",
     "grade": true,
     "grade_id": "cell-4dbc318b383bf70c",
     "locked": false,
     "points": 12,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "As $y_j = \\sum_{i=1}^d  x_{i} W_{ji} + b_j$,\n",
    "\n",
    "we have $\\frac{\\partial y_j}{\\partial W_ji}=x_i$, and $\\frac{\\partial y_j}{\\partial x_i}=W_ji$.\n",
    "\n",
    "Thus, \n",
    "\n",
    "$\\frac{\\partial J}{\\partial W_ji}=\\frac{\\partial J}{\\partial y_j}*\\frac{\\partial y_j}{\\partial W_ji}=\\delta j*x_i$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial x_i}=\\frac{\\partial J}{\\partial y_j}*\\frac{\\partial y_j}{\\partial x_i}=\\delta j*W_ji$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial b_j}=\\frac{\\partial J}{\\partial y_j}*\\frac{\\partial y_j}{\\partial b_j}=\\delta j$\n",
    "\n",
    "Then, \n",
    "\n",
    "$\\frac{\\partial J}{\\partial W}=\\delta x^T$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial x}=W^T*\\delta$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial b}=\\delta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ea3cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2815b900c97a10748f67524bf51b533a",
     "grade": false,
     "grade_id": "cell-920e213ee3ea40e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.6 (15 points WriteUp)\n",
    "\n",
    "We will find the derivatives for Conv layers now. Since most Deep Learning frameworks such as Pytorch, Tensorflow use cross-correlation in their respective \"convolution\" functions ([Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) and [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/convolution)), we will continue this abuse of notation. So the operation performed with the Conv Layer weights will be cross-correlation.\n",
    "    \n",
    "The input, $x$ is of shape $M\\times N$ with C channels. This will be *convolved* (actually cross-correlation) with $D$ number of $K\\times K$ filters, each with a bias term. The stride is 1 and there will be no padding. We know the gradient of some loss $J$ with respect to the output $y$, which will have $D$ channels. Show how to get $\\frac{\\partial J}{\\partial W}$, $\\frac{\\partial J}{\\partial x}$ and $\\frac{\\partial J}{\\partial b}$.\n",
    "\n",
    "The dimensions and notation are as follows:\n",
    "$$\n",
    "    \\frac{\\partial J}{\\partial y} = \\delta \\in \\mathbb{R}^{D\\times M_o \\times N_o}\n",
    "    \\quad\n",
    "    M_o = M-K+1\n",
    "    \\quad\n",
    "    N_o = N-K+1\n",
    "$$\n",
    "$$\n",
    "    x \\in \\mathbb{R}^{C\\times M \\times N}\n",
    "    \\quad\n",
    "    W \\in \\mathbb{R}^{D\\times C \\times K \\times K}\n",
    "    \\quad\n",
    "    b \\in \\mathbb{R}^{D}\n",
    "$$\n",
    "\n",
    "$x_{c, i, j}:$ The element at the $i^{th}$ row, the $j^{th}$ column and the $c^{th}$ channel of the input\n",
    "\n",
    "$y_{c, i, j}:$ The element at the $i^{th}$ row, the $j^{th}$ column and the $c^{th}$ channel of the output\n",
    "\n",
    "$W_{d, c, i, j}:$ The element at the $i^{th}$ row, the $j^{th}$ column, the $c^{th}$ channel of the kernel of the $d^{th}$ filter\n",
    "\n",
    "*For this question, you may compute the derivatives with scalars only. You don't need to re-form the matrix*\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217bc5ff",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d306d84ffe6763436b897502147ab18",
     "grade": true,
     "grade_id": "cell-84844fb51fed1aab",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "With $y_{d,i,j}=\\sum_c \\sum_p \\sum_q (W_{d,c,p,q}*X_{c,i+p,j+q})+b_d$,\n",
    "\n",
    "we can see the matirx $W_{d,c,p,q}$ is fixed and will be multiplied by $X_{c,i+p,j+q}$ for all i and j,\n",
    "\n",
    "Thus, we have $\\frac{\\partial J}{\\partial W_{d,c,p,q}} = \\frac{\\partial J}{\\partial y} \\frac{\\partial y}{\\partial W_{d,c,p,q}} = \\sum_i \\sum_j \\delta_{d,i,j} X_{c,i+p,j+q}$,\n",
    "\n",
    "where $X_c,k,l$ is fixed and multiplied by $W_{d,c,k-i,l-j}$ for all i, j and d.\n",
    "\n",
    "Thus, we have $\\frac{\\partial J}{\\partial X_{c,k,l}} = \\frac{\\partial J}{\\partial y} \\frac{\\partial y}{\\partial X_{c,k,l}} = \\sum_d \\sum_i \\sum_j \\delta_{d,i,j} W_{d,c,k-i,l-j}$.\n",
    "\n",
    "Then, with $d$ fixed, we can calculate $\\frac{\\partial J}{\\partial b_d}$,\n",
    "\n",
    "and sum over all $\\delta$,\n",
    "\n",
    "$\\frac{\\partial J}{\\partial b_d} = \\frac{\\partial J}{\\partial y_d} \\frac{\\partial y_d}{\\partial b_d} = \\sum_c \\sum_i \\sum_j \\delta_{d,i,j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4fabf4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "674f5af48cfacd5f2b47e7085dba8ae3",
     "grade": false,
     "grade_id": "cell-1e10040668b36463",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.7\n",
    "\n",
    "When the neural network applies the elementwise activation function (such as sigmoid), the gradient of the activation function scales the back-propagation update. This is directly from the chain rule, $\\frac{d}{d x} f(g(x)) = f'(g(x)) g'(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0275b3c9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7216e978dab46e7da90591e8b5eadcd4",
     "grade": false,
     "grade_id": "cell-d0948d91db6d6a34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.7.1 (1 point WriteUp)\n",
    "Consider the sigmoid activation function for deep neural networks. Why might it lead to a \"vanishing gradient\" problem if it is used for many layers (consider plotting the $\\sigma'(x)$ in Q1.4)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4278c157",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d20b744c87c16cff3860cb71abd8b5b7",
     "grade": true,
     "grade_id": "cell-d89dec7f9dd5635d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The derivatives of sigmoid function is close to 0.\n",
    "\n",
    "Thus, the when working on multiple-layer models, the gradient will reduce very quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd78443e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f84086fd60884f1666a86fc21b619897",
     "grade": false,
     "grade_id": "cell-ebcafd1b185b5253",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.7.2 (1 point WriteUp)\n",
    "Often it is replaced with $\\tanh(x) = \\frac{1-e^{-2x}}{1+e^{-2x}}$. What are the output ranges of both $\\tanh$ and sigmoid? Why might we prefer $\\tanh$ ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d940324a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c69505610f75e170201ac5810030f51",
     "grade": true,
     "grade_id": "cell-6ef0bff01cadddd2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The output range of $tanh$ is -1 to 1.\n",
    "\n",
    "And the output range of $sigmoid$ is 0 to 1.\n",
    "\n",
    "Since $tanh$ function is larger absolute values at most of the range than the $sigmoid$,\n",
    "\n",
    "it tends to solve the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa26b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c045a69bc896bfc119effaeac4fc27fa",
     "grade": false,
     "grade_id": "cell-210ca940cc6cf12f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.7.3 (1 point WriteUp)\n",
    "Why does $\\tanh(x)$ have less of a vanishing gradient problem? (plotting the derivatives helps! for reference: $\\tanh'(x) = 1 - \\tanh(x)^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03414d7f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67a3435ba44fcec113d0dd7c0a0f8013",
     "grade": true,
     "grade_id": "cell-8925f054fa7aeede",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The $tanh$ function has larger values around 0,\n",
    "\n",
    "which will lead to larger gradient values when working on multi-layer models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24221ba8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c739863fc4ff1b97f9a5f3903ddd925",
     "grade": false,
     "grade_id": "cell-b22c9cf732ba7c0b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.7.4 (1 point WriteUp)\n",
    "$\\tanh$ is a scaled and shifted version of the sigmoid. Show how $\\tanh(x)$ can be written in terms of $\\sigma(x)$. (*Hint: consider how to make it have the same range*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e1acfd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c71d5c8cbe29d4cf8846bb0508e636f",
     "grade": true,
     "grade_id": "cell-51d748c739cc28d1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$tanh(x)=\\frac{1-e^{-2x}}{1+e^{-2x}}$,\n",
    "\n",
    "$=\\frac{1-e^{-2x}}{1+e^{-2x}}+\\frac{1+e^{-2x}}{1+e^{-2x}}-1$,\n",
    "\n",
    "$=\\frac{2}{1+e^{-2x}}-1$,\n",
    "\n",
    "$=2 \\sigma (2x)-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48adfc1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d784dc196042141a43dd4d3ca1e02b1f",
     "grade": false,
     "grade_id": "cell-572b479fed175027",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "***\n",
    "## For the following questions, please find the instructions in the corresponding jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd695937",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c91869b98f9b4caae54948f1b46215e6",
     "grade": false,
     "grade_id": "cell-73a9363dbb4eeb8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q2 Implement a Fully Connected Network (65 points + 10 Extra Credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f107feb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ed7a931a26950507cb91fe9f3a20e74",
     "grade": false,
     "grade_id": "cell-61d6980ffe69f0cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.1.1 (3 points WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da6806",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "481f81209d087fc42c585a066c0d38b9",
     "grade": true,
     "grade_id": "cell-5adbb50e2e048b6c",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "If all network are initialized with zero, it will results in zero or close to zero gradients, which will lead to small or negligible update to weight and bias.\n",
    "\n",
    "The output will be very close to the input of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d8a04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "37da28bf7e7b703f770662d1b369983d",
     "grade": false,
     "grade_id": "cell-377494940d117ac8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.1.3 (2 points WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30968fff",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea48c62fb88f8e96cfcca6b4750590f6",
     "grade": true,
     "grade_id": "cell-cd2436d1298e6db3",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "We initialized with random numbers to have the hidden layers be updated in different scales.\n",
    "\n",
    "We scale the initialization depanding on layer size to maintain the variance for layers with different sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c715e54",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5dab96c7da53901d6339a2edd564f38",
     "grade": false,
     "grade_id": "cell-e4b82a4d357cc644",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q3 Training Models (20 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90a554",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c94c62abd3795b9cdd129d1a7bdd950",
     "grade": false,
     "grade_id": "cell-6489cea69bd8bdd3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q3.2 (3 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a63ff42",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e29bc06161593b30c7c37dd27ea29eba",
     "grade": true,
     "grade_id": "cell-e380768cc8511806",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "With learning rate = 1e-2\n",
    "\n",
    "<img src=\"screen shots/3.2-1.png\">\n",
    "\n",
    "With learning rate = 1e-3\n",
    "\n",
    "<img src=\"screen shots/3.2-2.png\">\n",
    "\n",
    "With learning rate = 1e-4\n",
    "\n",
    "<img src=\"screen shots/3.2-3.png\">\n",
    "\n",
    "The best validating accuracy is at around 1e-3 learning rate.\n",
    "\n",
    "At learning rate = 1e-2, the accuracy start to drop after 10 epoches,\n",
    "\n",
    "and at learning rate = 1e-4, the accuracy iters very slow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72841ff4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3478e43dd1a1e0d9e798ec5ca6bfe1f",
     "grade": false,
     "grade_id": "cell-9a360d4e2d2a59f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q3.3 (2 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371db3f9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b855fc6e16d07858a3ab4b060d605316",
     "grade": true,
     "grade_id": "cell-7cf3247bb27e0005",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Initial weight:\n",
    "\n",
    "<img src=\"screen shots/3.3-1.png\">\n",
    "\n",
    "Weight after training:\n",
    "\n",
    "<img src=\"screen shots/3.3-2.png\">\n",
    "\n",
    "After the training, the weight shows some patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8e037",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82b24e4243fdda57695057b7d4ce081d",
     "grade": false,
     "grade_id": "cell-c10943ecdbefccf2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q3.4 (3 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68bcb12",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d43991574656ba1a0eb1406923f56f2",
     "grade": true,
     "grade_id": "cell-d318551e7bb2f699",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<img src=\"screen shots/3.4-1.png\">\n",
    "\n",
    "From the image comparision, we can see the weight shows similar patterns with the actual letters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e52d12",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "388f963a9bb49f6382837fcc8ee45a96",
     "grade": false,
     "grade_id": "cell-9d5e6606a4d1e707",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q3.5 (4 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e2a03",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3f8d3fe3399fe704eacaf73425da961",
     "grade": true,
     "grade_id": "cell-699c18952c5f0ecf",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<img src=\"screen shots/3.5-1.png\">\n",
    "\n",
    "From the plot, we can see that \"O\" and \"0\", \"5\" and \"S\" are the most commly confused pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd606c8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48a7c17fe06eaa41b58095991a3cc574",
     "grade": false,
     "grade_id": "cell-234eebae3fbb6b14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q4 Extract Text from Images (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b1bf3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6605f95b8ece1b3f46f79b915dbe13b",
     "grade": false,
     "grade_id": "cell-564cb15873d70616",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.1 (3 points WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed9080",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e08a4d82b841f978434652fadd620ac4",
     "grade": true,
     "grade_id": "cell-371a94429864d47b",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. All characters must be seperate.\n",
    "\n",
    "2. All components of a single character must be connected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8c60d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4edb60ed50aae686c4440d4da43c7203",
     "grade": false,
     "grade_id": "cell-466d4c77e800b085",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.2 (13 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36408ef9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "527abea6466e5fad42c1f2d89dbb31ac",
     "grade": true,
     "grade_id": "cell-30d1533c7f070694",
     "locked": false,
     "points": 13,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "```python\n",
    "import numpy as np\n",
    "import skimage\n",
    "import skimage.measure\n",
    "import skimage.color\n",
    "import skimage.restoration\n",
    "import skimage.io\n",
    "import skimage.filters\n",
    "import skimage.morphology\n",
    "import skimage.segmentation\n",
    "\n",
    "# takes a color image\n",
    "# returns a list of bounding boxes and black_and_white image\n",
    "def findLetters(image):\n",
    "    bboxes = []\n",
    "    bw = None\n",
    "    # insert processing in here\n",
    "    # one idea estimate noise -> denoise -> greyscale -> threshold -> morphology -> label -> skip small boxes \n",
    "    # this can be 10 to 15 lines of code using skimage functions\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    img = skimage.color.rgb2gray(image)\n",
    "    thr = skimage.filters.threshold_otsu(img)\n",
    "    bw = skimage.morphology.closing(img <= thr, skimage.morphology.square(10))\n",
    "    clr = skimage.segmentation.clear_border(bw)\n",
    "    label_img = skimage.measure.label(clr)\n",
    "\n",
    "    for reg in skimage.measure.regionprops(label_img):\n",
    "        if (reg.area >= 100):\n",
    "            bboxes.append(reg.bbox)\n",
    "\n",
    "    # raise NotImplementedError()\n",
    "    return bboxes, bw\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6842b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "167e6bb80f063a680466df23c8ed3a27",
     "grade": false,
     "grade_id": "cell-a4ab802715e8fde6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.3 (6 points WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f60411b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "876f9ca6285b996ddfdfa544123ec302",
     "grade": true,
     "grade_id": "cell-d4a44957464f69b9",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<img src=\"screen shots/4.3-1.png\">\n",
    "\n",
    "<img src=\"screen shots/4.3-2.png\">\n",
    "\n",
    "<img src=\"screen shots/4.3-3.png\">\n",
    "\n",
    "<img src=\"screen shots/4.3-4.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09831f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3addd5c49e15d1c3a4db909e988d4cfa",
     "grade": false,
     "grade_id": "cell-2a94753cc27fccbe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.4 (13 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c186c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9abbef4396c44e65c1bd963d38631dbe",
     "grade": true,
     "grade_id": "cell-737f94592ee1fc0c",
     "locked": false,
     "points": 13,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "```\n",
    "01_list.jpg\n",
    "7QDOLIQT\n",
    "INAREATDDDLIQ8\n",
    "2LH2ERQEE8HER8RQ8\n",
    "8H8NGQNTQDQEIQ7\n",
    "3R2AE82EX0UM6EA6R6ADX\n",
    "EQMP6886DZ1H8NGQ\n",
    "5R8WARDX0URQE6EWD8B\n",
    "ANAR\n",
    "\n",
    "02_letters.jpg\n",
    "AB6DE8G\n",
    "HI8RLRN\n",
    "08QRQTU\n",
    "VWXXZ\n",
    "8Z3QS6383Q\n",
    "\n",
    "03_haiku.jpg\n",
    "HAIR6QAREGASX\n",
    "BQ7SQMETIMESTREXDDNTMAKESENQE\n",
    "RBFRIGERA8QR\n",
    "\n",
    "04_deep.jpg\n",
    "DEE8LEARBING\n",
    "DEEBERLBARNING\n",
    "DEERES8LEARRING\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d42d8cc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6bb22c308c16293123a8cb36258be42",
     "grade": false,
     "grade_id": "cell-e5cd95eec95a2803",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q5 Image Compression with Autoencoders [Extra Credit](25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212a33d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f785be92e6500cf812623c5f04def06e",
     "grade": false,
     "grade_id": "cell-8a21593a43945ffa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q5.1.1 [Extra Credit](10 points Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa1141",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d02ee49d26e0c7f7ec983ff8ed361c83",
     "grade": true,
     "grade_id": "cell-a14d388df159b04c",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5960d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "026c5dbdb601cd578f75ee1cf2370d98",
     "grade": false,
     "grade_id": "cell-6966f08a72f45ffb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q5.2 [Extra Credit](3 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48b5aa5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86cc58520dc956edf4e6ce2e6e7bbb97",
     "grade": true,
     "grade_id": "cell-a372ab8ed75b12ef",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f314e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a154ac361c62555f562ce6ead2afced",
     "grade": false,
     "grade_id": "cell-70958da3fad26122",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q5.3.1 [Extra Credit](4 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5a5c8a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "098d955e7a2b27bd750f607b22649f8f",
     "grade": true,
     "grade_id": "cell-7fa95d191289b087",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17edfab0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d821fa3a0e3d33936ed51e80b3b53437",
     "grade": false,
     "grade_id": "cell-ef6f9cc6c4e94fa5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q5.3.2 [Extra Credit](3 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d2611e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d51a06c464b2a3e64c918cd011f4c07",
     "grade": true,
     "grade_id": "cell-6ec226a41e5e9ec2",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde62bf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a643c6ea6bd559bcd4227e73b7fc6004",
     "grade": false,
     "grade_id": "cell-7d5e7253cd0fa4de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q6 Comparing against PCA [Extra Credit](15 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0870e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3acfc281c735af9489846f6a15d52b80",
     "grade": false,
     "grade_id": "cell-b9da236a37a91e2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6.1 [Extra Credit](4 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf78efd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ad8b69f5e20d8ca53ad73e0ecd2dac3",
     "grade": true,
     "grade_id": "cell-9454330fc70ecd95",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23ab825",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3369f26cb22d40444e83b4e1249201d3",
     "grade": false,
     "grade_id": "cell-1b7ec6b69ff5f438",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6.2 [Extra Credit](4 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee32d677",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e4f860c3b38646a09b8033ae6316174",
     "grade": true,
     "grade_id": "cell-95b567b5c8381e3b",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4b74f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f31d56d2e923ff65f46531901d89801",
     "grade": false,
     "grade_id": "cell-78bf5924af955143",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6.3 [Extra Credit](4 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91542536",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46fcbf9e5231f598b759b7c34f4d30d2",
     "grade": true,
     "grade_id": "cell-24b71dedcffa1338",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcfbac3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4e3df385828fa4f76edf8c4b0116b92",
     "grade": false,
     "grade_id": "cell-2f4c2ac42d375426",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6.4 [Extra Credit](3 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215afb7c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18577ce2a29bc7dedc1f08a41a1cf78c",
     "grade": true,
     "grade_id": "cell-27922e2c1160421c",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a97ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "896262237ed588dd156db42e7e06c08a",
     "grade": false,
     "grade_id": "cell-8fac1ca3ff947951",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q7 PyTorch (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798781ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce78f233d97e1f4541527a039a629fa1",
     "grade": false,
     "grade_id": "cell-03dd8625c09f576b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.1.1 (10 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b2e507",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e86a489666ac68fabc4f8d8bf152b612",
     "grade": true,
     "grade_id": "cell-5ffc6cb938d61d80",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "<img src=\"screen shots/7.1.1-3.png\">\n",
    "\n",
    "<img src=\"screen shots/7.1.1-2.png\">\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "#from ipynb.fs.defs.q2 import *\n",
    "import import_ipynb\n",
    "from q2 import *\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "train_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "valid_data = scipy.io.loadmat('data/nist36_valid.mat')\n",
    "test_data = scipy.io.loadmat('data/nist36_test.mat')\n",
    "\n",
    "train_x, train_y = torch.from_numpy(train_data['train_data']), torch.from_numpy(train_data['train_labels'])\n",
    "valid_x, valid_y = torch.from_numpy(valid_data['valid_data']), torch.from_numpy(valid_data['valid_labels'])\n",
    "test_x, test_y = torch.from_numpy(test_data['test_data']), torch.from_numpy(test_data['test_labels'])\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(train_x.shape[1], 64)\n",
    "        self.fc2 = torch.nn.Linear(64, train_y.shape[1])\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "train_batches = get_random_batches(train_x, train_y, 16)\n",
    "\n",
    "max_iters = 50\n",
    "train_loss_mem = []\n",
    "train_acc_mem = []\n",
    "valid_loss_mem = []\n",
    "valid_acc_mem = []\n",
    "\n",
    "for iters in range(max_iters):\n",
    "    for phase in ['train', 'val']:\n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        if phase == 'train':\n",
    "            batches = train_batches\n",
    "            model.train()\n",
    "        else:\n",
    "            batches = [(valid_x, valid_y)]\n",
    "            model.eval()\n",
    "\n",
    "        for xb, yb in batches:\n",
    "            inputs, labels = xb.float(), np.argmax(yb.float(), axis=1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        if phase == 'train':\n",
    "            print(\"Train Acc:\", correct / total, \" Train Loss:\", running_loss / total)\n",
    "            train_acc_mem.append(correct / total)\n",
    "            train_loss_mem.append(running_loss / total)\n",
    "        else:\n",
    "            print(\"Val Acc:\", correct / total, \" Val Loss:\", running_loss / total)\n",
    "            valid_acc_mem.append(correct / total)\n",
    "            valid_loss_mem.append(running_loss / total)\n",
    "\n",
    "print('Validation accuracy: ',correct / total)\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(train_acc_mem, label='Train Acc')\n",
    "plt.plot(valid_acc_mem, label='Valid Acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_loss_mem, label='Train Loss')\n",
    "plt.plot(valid_loss_mem, label='Valid Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# raise NotImplementedError()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c88cf1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69eafedd527ad9c3a8f669acddb315e4",
     "grade": false,
     "grade_id": "cell-b310f2ddba0f6647",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.1.2 (3 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e539c1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3cacd397b428d4eadd73ea230f56d108",
     "grade": true,
     "grade_id": "cell-f5a9affcedad4b2e",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "<img src=\"screen shots/7.1.2-1.png\">\n",
    "\n",
    "<img src=\"screen shots/7.1.2-2.png\">\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "import copy\n",
    "\n",
    "# Use GPU if available, otherwise stick with cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"device = {}\".format(device))\n",
    "\n",
    "print(\"Get dataset\")\n",
    "mnist_train = MNIST(root=\"data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "trainset_loader = DataLoader(mnist_train, batch_size=16, shuffle=True, num_workers=1)\n",
    "\n",
    "mnist_test = MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "testset_loader = DataLoader(mnist_test, batch_size=16, shuffle=True, num_workers=1)\n",
    "\n",
    "print(\"dataset size train, test\")\n",
    "print(trainset_loader.dataset.data.shape)\n",
    "print(testset_loader.dataset.data.shape)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Conv2d(1, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16*12*12, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv(x)))\n",
    "        x = x.view(-1, 16*12*12)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "Best_state = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0\n",
    "\n",
    "max_iters = 10\n",
    "train_loss_mem = []\n",
    "train_acc_mem = []\n",
    "valid_loss_mem = []\n",
    "valid_acc_mem = []\n",
    "\n",
    "for itr in range(max_iters):\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.\n",
    "        running_total = 0.\n",
    "        running_correct = 0.\n",
    "\n",
    "        if phase == 'train':\n",
    "            dataset_loader = trainset_loader\n",
    "        else:\n",
    "            dataset_loader = testset_loader\n",
    "\n",
    "        for i, data in enumerate(dataset_loader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            running_total += labels.size(0)\n",
    "            running_correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        if phase == 'train':\n",
    "            print(\"Train Acc:\", running_correct / running_total, \" Train Loss:\", running_loss / running_total)\n",
    "            train_acc_mem.append(running_correct / running_total)\n",
    "            train_loss_mem.append(running_loss / running_total)\n",
    "        else:\n",
    "            acc = running_correct / running_total\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            print(\"Val Acc:\", running_correct / running_total, \" Val Loss:\", running_loss / running_total)\n",
    "            valid_acc_mem.append(running_correct / running_total)\n",
    "            valid_loss_mem.append(running_loss / running_total)\n",
    "\n",
    "print('Testing accuracy: ',best_acc)\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(train_acc_mem, label='Train Acc')\n",
    "plt.plot(valid_acc_mem, label='Test Acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_loss_mem, label='Train Loss')\n",
    "plt.plot(valid_loss_mem, label='Test Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# raise NotImplementedError()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac5571",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10e4fe6fb3561465bf1d2be58eac4cd8",
     "grade": false,
     "grade_id": "cell-36d48a100a6ddb8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.1.3 (2 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ca9ee",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74715a525b6ea868aa46e9612811a809",
     "grade": true,
     "grade_id": "cell-1e34acb3833e499c",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "<img src=\"screen shots/7.1.3-1.png\">\n",
    "\n",
    "<img src=\"screen shots/7.1.3-1.png\">\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use GPU if available, otherwise stick with cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"device = {}\".format(device))\n",
    "\n",
    "print(\"Get dataset\")\n",
    "train_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "valid_data = scipy.io.loadmat('data/nist36_valid.mat')\n",
    "\n",
    "# train_x, train_y = train_data['train_data'].astype(np.float32), train_data['train_labels'].astype(np.int)\n",
    "# valid_x, valid_y = valid_data['valid_data'].astype(np.float32), valid_data['valid_labels'].astype(np.int)\n",
    "\n",
    "# To do\n",
    "\n",
    "train_x, train_y = torch.from_numpy(train_data['train_data']), torch.from_numpy(train_data['train_labels'])\n",
    "valid_x, valid_y = torch.from_numpy(valid_data['valid_data']), torch.from_numpy(valid_data['valid_labels'])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "train_batches = get_random_batches(train_x, train_y, 16)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 64)\n",
    "        self.fc2 = nn.Linear(64, train_y.shape[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "max_iters = 50\n",
    "train_loss_mem = []\n",
    "train_acc_mem = []\n",
    "valid_loss_mem = []\n",
    "valid_acc_mem = []\n",
    "\n",
    "for itr in range(max_iters):\n",
    "    for phase in ['train', 'val']:\n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        if phase == 'train':\n",
    "            batches = train_batches\n",
    "            model.train()\n",
    "        else:\n",
    "            batches = [(valid_x, valid_y)]\n",
    "            model.eval()\n",
    "\n",
    "        for xb, yb in batches:\n",
    "            inputs, labels = xb.float(), np.argmax(yb.float(), axis=1)\n",
    "            inputs = inputs.view(inputs.shape[0], 1, 32, 32)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        if phase == 'train':\n",
    "            print(\"Train Acc:\", correct / total, \" Train Loss:\", running_loss / total)\n",
    "            train_acc_mem.append(correct / total)\n",
    "            train_loss_mem.append(running_loss / total)\n",
    "        else:\n",
    "            print(\"Val Acc:\", correct / total, \" Val Loss:\", running_loss / total)\n",
    "            valid_acc_mem.append(correct / total)\n",
    "            valid_loss_mem.append(running_loss / total)\n",
    "\n",
    "print('validate accuracy: ',best_acc)\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(train_acc_mem, label='Train Acc')\n",
    "plt.plot(valid_acc_mem, label='Valid Acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_loss_mem, label='Train Loss')\n",
    "plt.plot(valid_loss_mem, label='Valid Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# raise NotImplementedError()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c82e6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7dbd040f3376e7d6a32cdcb0395dcce8",
     "grade": false,
     "grade_id": "cell-646081b13d109d59",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.1.4 (15 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9275d1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7224a041d0e860f947177e01f87d6729",
     "grade": true,
     "grade_id": "cell-506e3bc2f687b05d",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "```\n",
    "01_list.jpg\n",
    "rOOO7HWr\n",
    "4EtTMtrOOO7YNr\n",
    "UnInnTOTT4InTYnnr\n",
    "7IHSQOSrOOOgMNf\n",
    "WrnKTnnraDZZZn27aMeOX\n",
    "OOET7MrrOYFZHSQN\n",
    "4Ir3eaOXO2rnn7T3HrI\n",
    "eStO\n",
    "Accuracy: 0.09649122807017543\n",
    "\n",
    "02_letters.jpg\n",
    "tOODMTQ\n",
    "IHPT7ES\n",
    "OrQrNtZ\n",
    "D3XXn\n",
    "tgW4nOCnWQ\n",
    "Accuracy: 0.1388888888888889\n",
    "\n",
    "03_haiku.jpg\n",
    "ICHXDNC0MMCNr\n",
    "ODrNOEMrHEMNrZM2OOSrECTMNMSN9\n",
    "TnTOHQMaCFOa\n",
    "Accuracy: 0.05555555555555555\n",
    "\n",
    "04_deep.jpg\n",
    "OMTr7MtnSHIQ\n",
    "GrnLrJnngTSrSQ\n",
    "OrnKrNfGRKJSHSR\n",
    "Accuracy: 0.0\n",
    "```\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import EMNIST\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "import os\n",
    "import skimage\n",
    "import matplotlib\n",
    "# from q4.ipynb import *\n",
    "\n",
    "# Use GPU if available, otherwise stick with cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"device = {}\".format(device))\n",
    "\n",
    "print(\"Get dataset\")\n",
    "\n",
    "EMNIST.url = 'http://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip'\n",
    "# Reference for transform function\n",
    "# https://stackoverflow.com/a/54513835\n",
    "transform=torchvision.transforms.Compose([\n",
    "    lambda img: torchvision.transforms.functional.rotate(img, -90),\n",
    "    lambda img: torchvision.transforms.functional.hflip(img),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "emnist_train = EMNIST(root=\"data\", split='balanced', train=True, download=True, transform=transform)\n",
    "trainset_loader = DataLoader(emnist_train, batch_size=20, shuffle=True, num_workers=1)\n",
    "\n",
    "emnist_test = EMNIST(root=\"data\", split='balanced', train=False, download=True, transform=transform)\n",
    "testset_loader = DataLoader(emnist_test, batch_size=20, shuffle=True, num_workers=1)\n",
    "\n",
    "# Ref: https://github.com/gaurav0651/emnist/blob/master/train_emnist.ipynb\n",
    "label_map = np.array(['0','1','2','3','4','5','6','7','8','9',\n",
    "       'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z',\n",
    "       'a','b','d','e','f','g','h','n','q','r','t'])\n",
    "\n",
    "print(trainset_loader.dataset.data.shape)\n",
    "print(trainset_loader.dataset.data.shape)\n",
    "print(testset_loader.dataset.data.shape)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Conv2d(1, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 12 * 12, 128)\n",
    "        self.fc2 = nn.Linear(128, 47)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv(x)))\n",
    "        x = x.view(-1, 16 * 12 * 12)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "max_iters = 5\n",
    "\n",
    "for itr in range(max_iters):\n",
    "    loss_sum = 0\n",
    "    for i, data in enumerate(trainset_loader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        batch_imgs = inputs[:, 0, :, :].detach().numpy()\n",
    "        batch_imgs = batch_imgs.reshape(batch_imgs.shape[0], -1)\n",
    "        batch_imgs = -(batch_imgs - np.mean(batch_imgs, axis=1).reshape(-1, 1))/np.std(batch_imgs, axis=1).reshape(-1, 1)\n",
    "        inputs = torch.from_numpy(batch_imgs.reshape(batch_imgs.shape[0], 1, 28, 28))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum +=loss.item() * inputs.shape[0]\n",
    "\n",
    "    if itr % 1 == 0:\n",
    "        print(\"valid itr: {:02d} \\t loss: {:.2f}\".format(itr, loss_sum/train_size/len(trainset_loader.dataset)))\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(model.state_dict(), \"q7_1_4.pth\")\n",
    "\n",
    "# raise NotImplementedError()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74cf45f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4abc20e42d98374cefff61d298a72b58",
     "grade": false,
     "grade_id": "cell-08d1cfcc2156f6e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.2.1 (10 points WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397af2d5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16fb1e938c1dc10d26a79e0b2d19dcc5",
     "grade": true,
     "grade_id": "cell-f8bcdfeff7c2b335",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "<img src=\"screen shots/7.2.1-1.png\">\n",
    "\n",
    "<img src=\"screen shots/7.2.1-1.png\">\n",
    "\n",
    "```python\n",
    "# Code for fine-tune squeezenet1_1\n",
    "# YOUR CODE HERE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from nn import *\n",
    "# from q4 import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import copy\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "val_transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "flower17_trainset = datasets.ImageFolder(root='data/oxford-flowers17/train', transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(flower17_trainset, batch_size=8, shuffle=True, num_workers=4)\n",
    "flower17_valset = datasets.ImageFolder(root='data/oxford-flowers17/val', transform=val_transform)\n",
    "val_loader = torch.utils.data.DataLoader(flower17_valset, batch_size=8, shuffle=False, num_workers=4)\n",
    "flower17_testset = datasets.ImageFolder(root='data/oxford-flowers17/test', transform=val_transform)\n",
    "test_loader = torch.utils.data.DataLoader(flower17_testset, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 5)\n",
    "        self.fc1 = nn.Linear(32 * 24 * 24, 512)\n",
    "        self.fc2 = nn.Linear(512, 17)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 32 * 24 * 24)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def main():\n",
    "    model_name = \"fine_tuned\"\n",
    "    model = torchvision.models.squeezenet1_1(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.classifier[1] = nn.Conv2d(512, 17, kernel_size=(1, 1), stride=(1, 1))\n",
    "    model.num_classes = 17\n",
    "\n",
    "    params_to_update = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            params_to_update.append(param)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "    best_model_state = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    max_iters = 10\n",
    "    train_acc_history = []\n",
    "    train_loss_history = []\n",
    "    val_acc_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.\n",
    "            running_total = 0.\n",
    "            running_correct = 0.\n",
    "\n",
    "            if phase == 'train':\n",
    "                dataset_loader = train_loader\n",
    "            else:\n",
    "                dataset_loader = val_loader\n",
    "\n",
    "            for i, data in enumerate(dataset_loader, 0):\n",
    "                inputs, labels = data\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                running_total += labels.size(0)\n",
    "                running_correct += (predicted == labels).sum().item()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            if phase == 'train':\n",
    "                print(\"Train Acc:\", running_correct / running_total, \" Train Loss:\", running_loss / running_total)\n",
    "                train_acc_history.append(running_correct / running_total)\n",
    "                train_loss_history.append(running_loss / running_total)\n",
    "            else:\n",
    "                acc = running_correct / running_total\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                print(\"Val Acc:\", running_correct / running_total, \" Val Loss:\", running_loss / running_total)\n",
    "                val_acc_history.append(running_correct / running_total)\n",
    "                val_loss_history.append(running_loss / running_total)\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(train_acc_history, label='Training Acc')\n",
    "    plt.plot(val_acc_history, label='Val Acc')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(train_loss_history, label='Training Loss')\n",
    "    plt.plot(val_loss_history, label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    torch.save(best_model_state, \"q7_2_fine_tuned.pth\")\n",
    "    model.load_state_dict(torch.load(\"q7_2_fine_tuned.pth\"))\n",
    "\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print()\n",
    "    print(\"Test Accuracy:\", correct / total)\n",
    "\n",
    "# raise NotImplementedError()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d51a7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01537177398acd3fc733f5ecfc3a6f9d",
     "grade": false,
     "grade_id": "cell-d502ae43d1374766",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Appendix: Neural Network Overview\n",
    "Deep learning has quickly become one of the most applied machine learning techniques in computer vision. Convolutional neural networks have been applied to many different computer vision problems such as image classification, recognition, and segmentation with great success. In this assignment, you will first implement a fully connected feed forward neural network for hand written character classification. Then in the second part, you will implement a system to locate characters in an image, which you can then classify with your deep network. The end result will be a system that, given an image of hand written text, will output the text contained in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10b71b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e751aa5f73b563407f3df529cdc3eba1",
     "grade": false,
     "grade_id": "cell-4e449e493f11f2c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Basic Use\n",
    "Here we will give a brief overview of the math for a single hidden layer feed forward network. For a more detailed look at the math and derivation, please see the class slides.\n",
    "\n",
    "A fully-connected network $\\textbf{f}$, for classification, applies a series of linear and non-linear functions to an input data vector $\\textbf{x}$ of size $N\\times 1$ to produce an output vector $\\textbf{f}(\\textbf{x})$ of size $C\\times 1$, where each element $i$ of the output vector represents the probability of $\\textbf{x}$ belonging to the class $i$. Since the data samples are of dimensionality $N$, this means the input layer has $N$ input units. To compute the value of the output units, we must first compute the values of all the hidden layers. The first hidden layer *pre-activation* $\\textbf{a}^{(1)}(\\textbf{x})$ is given by\n",
    "\n",
    "$$\\textbf{a}^{(1)}(\\textbf{x}) = \\textbf{W}^{(1)}\\textbf{x} + \\textbf{b}^{(1)}$$\n",
    "\n",
    "Then the *post-activation* values of the first hidden layer $\\textbf{h}^{(1)}(\\textbf{x})$ are computed by applying a non-linear activation function $\\textbf{g}$ to the *pre-activation* values\n",
    "\n",
    "$$\\textbf{h}^{(1)}(\\textbf{x}) = \\textbf{g}(\\textbf{a}^{(1)}(\\textbf{x})) = \\textbf{g}(\\textbf{W}^{(1)}\\textbf{x} + \\textbf{b}^{(1)})$$\n",
    "\n",
    "Subsequent hidden layer ($1 < t \\leq T$) pre- and post activations are given by:\n",
    "\n",
    "$$\\textbf{a}^{(t)}(\\textbf{x}) = \\textbf{W}^{(t)}\\textbf{h}^{(t-1)} + \\textbf{b}^{(t)}$$\n",
    "\n",
    "$$\\textbf{h}^{(t)}(\\textbf{x}) = \\textbf{g}(\\textbf{a}^{(t)}(\\textbf{x}))$$\n",
    "\n",
    "The output layer *pre-activations* $\\textbf{a}^{(T)}(\\textbf{x})$ are computed in a similar way\n",
    "\n",
    "$$\\textbf{a}^{(T)}(\\textbf{x}) = \\textbf{W}^{(T)}\\textbf{h}^{(T-1)}(\\textbf{x}) + \\textbf{b}^{(T)}$$\n",
    "\n",
    "and finally the \\emph{post-activation} values of the output layer are computed with\n",
    "$$\\textbf{f}(\\textbf{x}) = \\textbf{o}(\\textbf{a}^{(T)}(\\textbf{x})) = \\textbf{o}(\\textbf{W}^{(T)}\\textbf{h}^{(T-1)}(\\textbf{x}) + \\textbf{b}^{(T)})$$\n",
    "\n",
    "where $\\textbf{o}$ is the output activation function. Please note the difference between $\\textbf{g}$ and $\\textbf{o}$! \n",
    "For this assignment, we will be using the sigmoid activation function for the hidden layer, so:\n",
    "$$\\textbf{g}(y) = \\frac{1}{1+\\exp(-y)}$$\n",
    "where when $\\textbf{g}$ is applied to a vector, it is applied element wise across the vector.\n",
    "\n",
    "Since we are using this deep network for classification, a common output activation function to use is the softmax function. This will allow us to turn the real value, possibly negative values of $\\textbf{a}^{(T)}(\\textbf{x})$ into a set of probabilities (vector of positive numbers that sum to 1). Letting $\\textbf{x}_i$ denote the $i^{th}$ element of the vector $\\textbf{x}$, the softmax function is defined as:\n",
    "$$\\textbf{o}_i(\\textbf{y}) = \\frac{\\exp(\\textbf{y}_i)}{\\sum_j \\exp(\\textbf{y}_j)}$$\n",
    "\n",
    "![](figures/letter_montage.jpg)\n",
    "<center>Samples from NIST Special 19  dataset</center>\n",
    "\n",
    "\n",
    "Gradient descent is an iterative optimisation algorithm, used to find the local optima. To find the local minima, we start at a point on the function and move in the direction of negative gradient (steepest descent) till some stopping criteria is met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a57dc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "917c876a76a8dd2304785000bd206f6a",
     "grade": false,
     "grade_id": "cell-c6f5b4cd57160fca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Backprop\n",
    "The update equation for a general weight $W^{(t)}_{ij}$ and bias $b^{(t)}_i$ is\n",
    "$$\n",
    "W^{(t)}_{ij} = W^{(t)}_{ij} - \\alpha*\\frac{\\partial L_{\\textbf{f}}}{\\partial W^{(t)}_{ij}}(\\textbf{x})\\hspace{1cm}\n",
    "b^{(t)}_{i} = b^{(t)}_{i} - \\alpha*\\frac{\\partial L_{\\textbf{f}}}{\\partial b^{(t)}_{i}}(\\textbf{x})\n",
    "$$\n",
    "$\\alpha$ is the learning rate. Please refer to the back-propagation slides for more details on how to derive the gradients. Note that here we are using softmax loss (which is different from the least square loss in the slides)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fecfbd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec744557890bf229ea3a4f21a9b3b4e8",
     "grade": false,
     "grade_id": "cell-67800a69b8b95457",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## References\n",
    "\n",
    "[1]  Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. 2010. http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf.\n",
    "\n",
    "[2]  P. J. Grother. Nist special database 19 – handprinted forms and characters database. https://www.nist.gov/srd/nist-special-database-19, 1995."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
